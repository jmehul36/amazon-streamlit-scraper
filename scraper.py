# -*- coding: utf-8 -*-
"""scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U1oV8LhWI4Df6l70BLHoACugSaPH-DKM
"""

from bs4 import BeautifulSoup
import requests
import csv
import os
import time

CSV_FILE = "out.csv"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Connection': 'keep-alive',
    'DNT': '1',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-User': '?1',
    'Sec-Fetch-Dest': 'document'
}
FIELDS = ["ASIN", "Title", "Brand", "Price", "Rating", "Review Count", "Availability", "Image URL", "Category", "Description"]

def write_headers():
    if not os.path.exists(CSV_FILE) or os.path.getsize(CSV_FILE) == 0:
        with open(CSV_FILE, "w", newline='', encoding='utf-8') as f:
            csv.writer(f).writerow(FIELDS)

def extract_asin(url):
    for key in ["/dp/", "/gp/product/"]:
        if key in url:
            return url.split(key)[1].split("/")[0]
    return "NA"

def get_text(soup, selector):
    tag = soup.select_one(selector)
    return tag.get_text(strip=True).replace(',', '') if tag else "NA"

def scrape_amazon_product(url):
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "lxml")
    except Exception as e:
        print(f"‚ùå Failed to fetch URL: {e}")
        return

    data = {
        "ASIN": extract_asin(url),
        "Title": get_text(soup, "span#productTitle"),
        "Brand": get_text(soup, "a#bylineInfo"),
        "Price": get_text(soup, "span#priceblock_ourprice") or get_text(soup, "span#priceblock_dealprice"),
        "Rating": get_text(soup, "span.a-icon-alt"),
        "Review Count": get_text(soup, "span#acrCustomerReviewText"),
        "Availability": get_text(soup, "div#availability span"),
        "Image URL": soup.select_one("#imgTagWrapperId img")['src'] if soup.select_one("#imgTagWrapperId img") else "NA",
        "Category": " > ".join([c.get_text(strip=True) for c in soup.select("a.a-link-normal.a-color-tertiary")]) or "NA",
        "Description": " | ".join([b.get_text(strip=True).replace(',', '') for b in soup.select("div#feature-bullets ul li span")]) or "NA"
    }

    print("\nüîç Product Info:")
    for key in FIELDS:
        print(f"{key}: {data[key]}")

    with open(CSV_FILE, "a", newline='', encoding='utf-8') as f:
        csv.writer(f).writerow([data[key] for key in FIELDS])
    print("‚úÖ Saved to out.csv\n")

if __name__ == "__main__":
    write_headers()
    print("üîé Enter Amazon product URLs (type 'done' to finish):")
    while True:
        url = input("Enter URL: ").strip()
        if url.lower() == "done":
            print("‚úÖ Done. Output saved to out.csv.")
            break
        elif url.startswith("http"):
            main(url)
            time.sleep(2)  # Be polite to Amazon
        else:
            print("‚ö†Ô∏è Invalid URL. Try again.")
